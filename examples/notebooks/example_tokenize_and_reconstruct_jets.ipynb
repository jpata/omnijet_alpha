{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import vector\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import gabbro\n",
    "import gabbro.plotting.utils as plot_utils\n",
    "from gabbro.plotting.feature_plotting import plot_features\n",
    "from gabbro.utils.arrays import ak_select_and_preprocess\n",
    "import tqdm\n",
    "\n",
    "# hacky way to setup logging in jupyter\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.info(\"Setup complete\")\n",
    "\n",
    "vector.register_awkward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitialize_p4(p4_obj: ak.Array):\n",
    "    if \"tau\" in p4_obj.fields:\n",
    "        p4 = vector.awk(\n",
    "            ak.zip(\n",
    "                {\n",
    "                    \"mass\": p4_obj.tau,\n",
    "                    \"x\": p4_obj.x,\n",
    "                    \"y\": p4_obj.y,\n",
    "                    \"z\": p4_obj.z,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        p4 = vector.awk(\n",
    "            ak.zip(\n",
    "                {\n",
    "                    \"energy\": p4_obj.t,\n",
    "                    \"x\": p4_obj.x,\n",
    "                    \"y\": p4_obj.y,\n",
    "                    \"z\": p4_obj.z,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return p4\n",
    "\n",
    "def deltaPhi(phi1, phi2):\n",
    "    diff = phi1 - phi2\n",
    "    return np.arctan2(np.sin(diff), np.cos(diff))\n",
    "\n",
    "def deltaR_etaPhi(eta1, phi1, eta2, phi2):\n",
    "    deta = np.abs(eta1 - eta2)\n",
    "    dphi = deltaPhi(phi1, phi2)\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "def deltaEta(eta1, eta2):\n",
    "    return np.abs(eta1 - eta2)\n",
    "\n",
    "def stack_and_pad_features(cand_features, max_cands):\n",
    "    cand_features_tensors = np.stack([ak.pad_none(cand_features[feat], max_cands, clip=True) for feat in cand_features.fields], axis=-1)\n",
    "    cand_features_tensors = ak.to_numpy(ak.fill_none(cand_features_tensors, 0))\n",
    "    # Swapping the axes such that it has the shape of (nJets, nFeatures, nParticles)\n",
    "    cand_features_tensors = np.swapaxes(cand_features_tensors, 1, 2)\n",
    "\n",
    "    cand_features_tensors[np.isnan(cand_features_tensors)] = 0\n",
    "    cand_features_tensors[np.isinf(cand_features_tensors)] = 0\n",
    "    return cand_features_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with the VQ-VAE\n",
    "\n",
    "This notebook provides a short example on how to\n",
    "\n",
    "- load a trained VQ-VAE model that was trained with this repo\n",
    "- use the model to encode/tokenize jets from the JetClass dataset\n",
    "- reconstruct/decode the jets from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the tokenizer model from checkpoint, and also get the feature_dict from the config ---\n",
    "\n",
    "from gabbro.models.vqvae import VQVAELightning\n",
    "\n",
    "# this checkpoint is the checkpoint from a tokenization training\n",
    "ckpt_path = \"../../checkpoints/vqvae_8192_tokens/model_ckpt.ckpt\"\n",
    "vqvae_model = VQVAELightning.load_from_checkpoint(ckpt_path).to(device)\n",
    "vqvae_model.eval()\n",
    "\n",
    "cfg = OmegaConf.load(Path(ckpt_path).parent / \"config.yaml\")\n",
    "pp_dict = OmegaConf.to_container(cfg.data.dataset_kwargs_common.feature_dict)\n",
    "print(\"\\npp_dict:\")\n",
    "for item in pp_dict:\n",
    "    print(item, pp_dict[item])\n",
    "\n",
    "# get the cuts from the pp_dict (since this leads to particles being removed during\n",
    "# preprocessing/tokenization), thus we also have to remove them from the original jets\n",
    "# when we compare the tokenized+reconstructed particles to the original ones)\n",
    "pp_dict_cuts = {\n",
    "    feat_name: {\n",
    "        criterion: pp_dict[feat_name].get(criterion)\n",
    "        for criterion in [\"larger_than\", \"smaller_than\"]\n",
    "    }\n",
    "    for feat_name in pp_dict\n",
    "}\n",
    "\n",
    "print(\"\\npp_dict_cuts:\")\n",
    "for item in pp_dict_cuts:\n",
    "    print(item, pp_dict_cuts[item])\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "print(vqvae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((8,32,3))\n",
    "mask = torch.randn((8,32)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_particle_reco, vq_out = vqvae_model.model.forward(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_out[\"q\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and reconstruct the jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_data = ak.from_parquet(\"/local/joosep/ml-tau-en-reg/ntuples/20240701_lowered_ptcut_merged/z_train.parquet\")\n",
    "tau_data = tau_data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_p4 = reinitialize_p4(tau_data[\"reco_cand_p4s\"])\n",
    "jet_p4 = reinitialize_p4(tau_data[\"reco_jet_p4s\"])\n",
    "tau_data_transf = ak.Array({\n",
    "    \"part_pt\": part_p4.pt,\n",
    "    \"part_etarel\": part_p4.eta - jet_p4.eta,\n",
    "    \"part_phirel\": deltaPhi(part_p4.phi, jet_p4.phi),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = filter(lambda p: p.requires_grad, vqvae_model.parameters())\n",
    "num_trainable_weights = sum([np.prod(p.size()) for p in model_params])\n",
    "num_trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization and reconstruction\n",
    "\n",
    "part_features_ak_tokenized = vqvae_model.tokenize_ak_array(\n",
    "    ak_arr=tau_data_transf,\n",
    "    pp_dict=pp_dict,\n",
    "    batch_size=512,\n",
    "    pad_length=128,\n",
    ")\n",
    "# note that if you want to reconstruct tokens from the generative model, you'll have\n",
    "# to remove the start token from the tokenized array, and subtract 1 from the tokens\n",
    "# (since we chose the convention to use 0 as the start token, so the tokens from the\n",
    "# generative model are shifted by 1 compared to the ones from the VQ-VAE)\n",
    "part_features_ak_reco = vqvae_model.reconstruct_ak_tokens(\n",
    "    tokens_ak=part_features_ak_tokenized,\n",
    "    pp_dict=pp_dict,\n",
    "    batch_size=512,\n",
    "    pad_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the tokenized and reconstructed jets\n",
    "print(\"First 5 tokenized jets:\")\n",
    "for i in range(5):\n",
    "    print(part_features_ak_tokenized[i])\n",
    "\n",
    "print(\"\\nFirst 5 reconstructed jets:\")\n",
    "for i in range(5):\n",
    "    print(part_features_ak_reco[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.num(tau_data_transf.part_pt)-ak.num(part_features_ak_tokenized), bins=np.linspace(0,10,11));\n",
    "plt.xticks(np.linspace(0,10,11))\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Nptcls - Ntokens\")\n",
    "plt.ylabel(\"jets / bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the four-momentum of the reconstructed jets and make comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p4s_from_part_features(part_features):\n",
    "    \"\"\"Small helper function to get the 4-momentum from part_features.\"\"\"\n",
    "    return ak.zip(\n",
    "        {\n",
    "            \"pt\": part_features.part_pt,\n",
    "            \"eta\": part_features.part_etarel,\n",
    "            \"phi\": part_features.part_phirel,\n",
    "            \"mass\": ak.zeros_like(part_features.part_pt),  # massless particles\n",
    "        },\n",
    "        with_name=\"Momentum4D\",\n",
    "    )\n",
    "\n",
    "p4s_original = get_p4s_from_part_features(tau_data_transf)\n",
    "p4s_reco = get_p4s_from_part_features(part_features_ak_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-3,3,100)\n",
    "plt.hist(ak.flatten(tau_data_transf.part_phirel), bins=b, histtype=\"step\");\n",
    "plt.hist(ak.flatten(part_features_ak_reco.part_phirel), bins=b, histtype=\"step\");\n",
    "#plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4s_original[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4s_reco[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inclusive jet-level distributions\n",
    "p4s_jets_original = ak.sum(p4s_original, axis=1)\n",
    "p4s_jets_reco = ak.sum(p4s_reco, axis=1)\n",
    "\n",
    "fig, axarr = plot_features(\n",
    "    ak_array_dict={\n",
    "        \"Original jets\": p4s_jets_original,\n",
    "        \"Reconstructed jets\": p4s_jets_reco,\n",
    "    },\n",
    "    names={\n",
    "        \"pt\": plot_utils.DEFAULT_LABELS[\"jet_pt\"],\n",
    "        \"eta\": plot_utils.DEFAULT_LABELS[\"jet_eta\"],\n",
    "        \"phi\": plot_utils.DEFAULT_LABELS[\"jet_phi\"],\n",
    "        \"mass\": plot_utils.DEFAULT_LABELS[\"jet_mass\"],\n",
    "    },\n",
    "    flatten=False,\n",
    "    decorate_ax_kwargs={\"yscale\": 1.7},\n",
    "    bins_dict={\n",
    "        \"pt\": np.linspace(0, 200, 100),\n",
    "        \"eta\": np.linspace(-0.1, 0.1, 100),\n",
    "        \"phi\": np.linspace(-0.1, 0.1, 100),\n",
    "        \"mass\": np.linspace(0, 10, 100),\n",
    "    },\n",
    ")\n",
    "# plot the resolution (i.e. jet features of the reconstructed jets - jet features of the original jets)\n",
    "fig, axarr = plot_features(\n",
    "    ak_array_dict={\n",
    "        \"Difference\": ak.Array(\n",
    "            {\n",
    "                \"pt\": p4s_jets_reco.pt - p4s_jets_original.pt,\n",
    "                \"eta\": p4s_jets_reco.eta - p4s_jets_original.eta,\n",
    "                \"phi\": deltaPhi(p4s_jets_reco.phi, p4s_jets_original.phi),\n",
    "                \"mass\": p4s_jets_reco.mass - p4s_jets_original.mass,\n",
    "            }\n",
    "        )\n",
    "    },\n",
    "    names={\n",
    "        \"pt\": \"Jet $p_T^{\\\\text{reco}} - p_T^{\\\\text{orig}}$\",\n",
    "        \"eta\": \"Jet $\\\\eta^{\\\\text{reco}} - \\\\eta^{\\\\text{orig}}$\",\n",
    "        \"phi\": \"Jet $\\\\phi^{\\\\text{reco}} - \\\\phi^{\\\\text{orig}}$\",\n",
    "        \"mass\": \"Jet $m^{\\\\text{reco}} - m^{\\\\text{orig}}$\",\n",
    "    },\n",
    "    flatten=False,\n",
    "    decorate_ax_kwargs={\"yscale\": 1.7},\n",
    "    bins_dict={\n",
    "        \"pt\": np.linspace(-15, 15, 100),\n",
    "        \"eta\": np.linspace(-0.05, 0.05, 100),\n",
    "        \"phi\": np.linspace(-0.05, 0.05, 100),\n",
    "        \"mass\": np.linspace(-5, 5, 100),\n",
    "    },\n",
    "    colors=[\"C2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-0.1, 0.1, 100)\n",
    "plt.hist(p4s_jets_original.phi, bins=b, histtype=\"step\", lw=1);\n",
    "plt.hist(p4s_jets_reco.phi, bins=b, histtype=\"step\", lw=1);\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "loaded_model = torch.load(\"../../checkpoints/generative_8192_tokens/OmniJet_generative_model_UnintentionalPinscher_59.ckpt\", map_location=torch.device('cpu'))\n",
    "from gabbro.models.gpt_model import BackboneModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_model = BackboneModel(256, 0.0, 8194, 128, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_state = {k.replace(\"module.\", \"\"): v for k, v in loaded_model[\"state_dict\"].items() if k.startswith(\"module.\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_model.load_state_dict(gpt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_features_ak_tokenized[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jets_padded_tokenized = ak.fill_none(ak.pad_none(part_features_ak_tokenized[0:8], 32), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jets_batch = ak.to_regular(jets_padded_tokenized)\n",
    "jets_batch = torch.tensor(jets_batch).long()\n",
    "padding_mask = jets_batch==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_jets = bb_model(jets_batch, padding_mask=padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_jets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
